#!/usr/bin/env python3
"""
A-Qlegal 2.0 - Comprehensive Training Script
Trains the system with all available legal data
"""

import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from loguru import logger
import torch
from transformers import (
    AutoTokenizer, AutoModel, 
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
from sentence_transformers import SentenceTransformer
import faiss
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pickle
from tqdm import tqdm
import re

# Configure logging
logger.remove()
logger.add("logs/training.log", level="DEBUG", format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {message}")
logger.add(lambda msg: print(f"\033[92m{msg}\033[0m"), level="INFO", format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}")

class AQlegalTrainer:
    def __init__(self):
        self.data_dir = Path("data")
        self.models_dir = Path("models")
        self.embeddings_dir = Path("data/embeddings")
        self.processed_dir = Path("data/processed")
        
        # Create directories
        self.models_dir.mkdir(exist_ok=True)
        self.embeddings_dir.mkdir(exist_ok=True)
        self.processed_dir.mkdir(exist_ok=True)
        
        # Initialize components
        self.legal_documents = []
        self.qa_pairs = []
        self.embeddings = None
        self.vectorizer = None
        self.sentence_model = None
        
    def load_all_data(self):
        """Load all available legal data from various sources"""
        logger.info("üîÑ Loading all legal data...")
        
        # 1. Load expanded legal dataset
        try:
            with open(self.data_dir / "expanded_legal_dataset.json", "r", encoding="utf-8") as f:
                expanded_data = json.load(f)
            logger.info(f"‚úÖ Loaded {len(expanded_data)} documents from expanded dataset")
            self.legal_documents.extend(expanded_data)
        except Exception as e:
            logger.warning(f"‚ùå Failed to load expanded dataset: {e}")
        
        # 2. Load enhanced legal documents
        try:
            with open(self.data_dir / "enhanced_legal" / "enhanced_legal_documents.json", "r", encoding="utf-8") as f:
                enhanced_data = json.load(f)
            logger.info(f"‚úÖ Loaded {len(enhanced_data)} documents from enhanced dataset")
            self.legal_documents.extend(enhanced_data)
        except Exception as e:
            logger.warning(f"‚ùå Failed to load enhanced dataset: {e}")
        
        # 3. Load world class legal data
        world_class_dirs = [
            "criminal_law", "civil_law", "constitutional_amendments",
            "high_court_decisions", "indian_constitution", "legal_glossary",
            "legal_precedents", "multilingual_legal", "supreme_court_judgments"
        ]
        
        for dir_name in world_class_dirs:
            try:
                file_path = self.data_dir / "world_class_legal" / dir_name / f"{dir_name}.json"
                if file_path.exists():
                    with open(file_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                    logger.info(f"‚úÖ Loaded {len(data)} documents from {dir_name}")
                    self.legal_documents.extend(data)
            except Exception as e:
                logger.warning(f"‚ùå Failed to load {dir_name}: {e}")
        
        # 4. Load Indian legal data
        indian_legal_files = [
            "fundamental_rights.json", "indian_constitution.json",
            "indian_legal_qa_pairs.json", "legal_documents.json", "legal_glossary.json"
        ]
        
        for file_name in indian_legal_files:
            try:
                file_path = self.data_dir / "indian_legal" / file_name
                if file_path.exists():
                    with open(file_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                    logger.info(f"‚úÖ Loaded {len(data)} documents from {file_name}")
                    self.legal_documents.extend(data)
            except Exception as e:
                logger.warning(f"‚ùå Failed to load {file_name}: {e}")
        
        # 5. Load external datasets
        try:
            # HuggingFace datasets
            hf_path = self.data_dir / "external_datasets" / "huggingface" / "indian_law_dataset"
            if (hf_path / "train.json").exists():
                with open(hf_path / "train.json", "r", encoding="utf-8") as f:
                    hf_data = json.load(f)
                logger.info(f"‚úÖ Loaded {len(hf_data)} documents from HuggingFace dataset")
                self.legal_documents.extend(hf_data)
        except Exception as e:
            logger.warning(f"‚ùå Failed to load HuggingFace dataset: {e}")
        
        logger.info(f"üìä Total documents loaded: {len(self.legal_documents)}")
        return len(self.legal_documents)
    
    def preprocess_documents(self):
        """Preprocess and standardize all legal documents"""
        logger.info("üîÑ Preprocessing legal documents...")
        
        processed_docs = []
        
        for doc in tqdm(self.legal_documents, desc="Processing documents"):
            try:
                # Extract key information
                processed_doc = {
                    "id": doc.get("id", f"doc_{len(processed_docs)}"),
                    "title": doc.get("title", doc.get("section", "Unknown")),
                    "content": doc.get("content", doc.get("text", "")),
                    "category": doc.get("category", "general"),
                    "section": doc.get("section", ""),
                    "punishment": doc.get("punishment", ""),
                    "citations": doc.get("citations", []),
                    "source": doc.get("source", "Unknown")
                }
                
                # Clean and normalize content
                processed_doc["content"] = self.clean_text(processed_doc["content"])
                
                # Generate simplified summary
                processed_doc["simplified_summary"] = self.generate_simplified_summary(
                    processed_doc["content"], processed_doc["title"]
                )
                
                # Extract keywords
                processed_doc["keywords"] = self.extract_keywords(processed_doc["content"])
                
                # Generate real-life example
                processed_doc["real_life_example"] = self.generate_real_life_example(
                    processed_doc["content"], processed_doc["title"]
                )
                
                processed_docs.append(processed_doc)
                
            except Exception as e:
                logger.warning(f"‚ùå Failed to process document: {e}")
                continue
        
        self.legal_documents = processed_docs
        logger.info(f"‚úÖ Processed {len(processed_docs)} documents")
        
        # Save processed data
        with open(self.processed_dir / "all_legal_documents.json", "w", encoding="utf-8") as f:
            json.dump(processed_docs, f, indent=2, ensure_ascii=False)
        
        return len(processed_docs)
    
    def clean_text(self, text):
        """Clean and normalize text"""
        if not text:
            return ""
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove special characters but keep legal citations
        text = re.sub(r'[^\w\s\.\,\;\:\!\?\(\)\[\]\-\'\"\/]', '', text)
        
        # Normalize legal citations
        text = re.sub(r'Section\s+(\d+[A-Z]?)', r'Section \1', text)
        
        return text.strip()
    
    def generate_simplified_summary(self, content, title):
        """Generate simplified summary of legal text"""
        if not content:
            return "No content available"
        
        # Simple rule-based summarization
        sentences = content.split('.')
        
        # Look for key phrases
        key_phrases = [
            "punishment", "penalty", "fine", "imprisonment", "jail",
            "offence", "crime", "violation", "prohibited", "forbidden"
        ]
        
        important_sentences = []
        for sentence in sentences:
            if any(phrase in sentence.lower() for phrase in key_phrases):
                important_sentences.append(sentence.strip())
        
        if important_sentences:
            return '. '.join(important_sentences[:3]) + '.'
        else:
            # Take first few sentences
            return '. '.join(sentences[:2]) + '.'
    
    def extract_keywords(self, content):
        """Extract keywords from legal content"""
        if not content:
            return []
        
        # Legal keywords
        legal_keywords = [
            "punishment", "fine", "imprisonment", "offence", "crime",
            "violation", "prohibited", "forbidden", "penalty", "jail",
            "court", "judge", "law", "legal", "criminal", "civil",
            "constitution", "rights", "duty", "liability", "damages"
        ]
        
        content_lower = content.lower()
        found_keywords = [kw for kw in legal_keywords if kw in content_lower]
        
        # Add section numbers
        section_matches = re.findall(r'Section\s+(\d+[A-Z]?)', content)
        found_keywords.extend([f"Section {s}" for s in section_matches])
        
        return list(set(found_keywords))[:10]  # Limit to 10 keywords
    
    def generate_real_life_example(self, content, title):
        """Generate real-life example for legal concept"""
        if not content:
            return "No example available"
        
        # Simple template-based example generation
        if "fraud" in content.lower() or "cheat" in content.lower():
            return "A person sells fake gold jewelry claiming it's real gold, tricking customers into paying high prices."
        elif "murder" in content.lower() or "homicide" in content.lower():
            return "A person intentionally kills another person with a weapon."
        elif "theft" in content.lower() or "steal" in content.lower():
            return "A person takes someone else's property without permission."
        elif "assault" in content.lower():
            return "A person physically attacks another person causing injury."
        elif "defamation" in content.lower():
            return "A person spreads false rumors about someone to damage their reputation."
        else:
            return f"Example related to {title}: A person violates this law and faces consequences."
    
    def create_qa_pairs(self):
        """Create question-answer pairs from legal documents"""
        logger.info("üîÑ Creating Q&A pairs...")
        
        qa_pairs = []
        
        for doc in tqdm(self.legal_documents, desc="Creating Q&A pairs"):
            try:
                # Generate questions based on document content
                questions = self.generate_questions(doc)
                
                for question in questions:
                    qa_pairs.append({
                        "question": question,
                        "answer": doc["simplified_summary"],
                        "context": doc["content"],
                        "section": doc.get("section", ""),
                        "category": doc["category"],
                        "source": doc["source"]
                    })
                    
            except Exception as e:
                logger.warning(f"‚ùå Failed to create Q&A for document: {e}")
                continue
        
        self.qa_pairs = qa_pairs
        logger.info(f"‚úÖ Created {len(qa_pairs)} Q&A pairs")
        
        # Save Q&A pairs
        with open(self.processed_dir / "legal_qa_pairs.json", "w", encoding="utf-8") as f:
            json.dump(qa_pairs, f, indent=2, ensure_ascii=False)
        
        return len(qa_pairs)
    
    def generate_questions(self, doc):
        """Generate questions from legal document"""
        questions = []
        
        title = doc["title"]
        section = doc.get("section", "")
        
        # Basic questions
        if section:
            questions.append(f"What is {section}?")
            questions.append(f"Explain {section}")
            questions.append(f"Tell me about {section}")
        
        questions.append(f"What is {title}?")
        questions.append(f"Explain {title}")
        
        # Category-specific questions
        category = doc["category"]
        if "criminal" in category.lower():
            questions.append(f"What is the punishment for {title}?")
            questions.append(f"Is {title} a crime?")
        elif "civil" in category.lower():
            questions.append(f"What are the civil implications of {title}?")
        elif "constitutional" in category.lower():
            questions.append(f"What does the Constitution say about {title}?")
        
        return questions[:3]  # Limit to 3 questions per document
    
    def train_sentence_transformer(self):
        """Train sentence transformer for semantic search"""
        logger.info("üîÑ Training sentence transformer...")
        
        try:
            # Load pre-trained model
            self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
            
            # Prepare training data
            texts = []
            for doc in self.legal_documents:
                texts.append(doc["content"])
                texts.append(doc["simplified_summary"])
            
            # Generate embeddings
            logger.info("Generating embeddings...")
            embeddings = self.sentence_model.encode(texts, show_progress_bar=True)
            
            # Save embeddings
            np.save(self.embeddings_dir / "sentence_embeddings.npy", embeddings)
            
            # Save model
            self.sentence_model.save(str(self.models_dir / "sentence_transformer"))
            
            logger.info("‚úÖ Sentence transformer trained and saved")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to train sentence transformer: {e}")
            return False
    
    def create_faiss_index(self):
        """Create FAISS index for fast similarity search"""
        logger.info("üîÑ Creating FAISS index...")
        
        try:
            # Load embeddings
            embeddings = np.load(self.embeddings_dir / "sentence_embeddings.npy")
            
            # Create FAISS index
            dimension = embeddings.shape[1]
            index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
            
            # Normalize embeddings for cosine similarity
            faiss.normalize_L2(embeddings)
            index.add(embeddings)
            
            # Save index
            faiss.write_index(index, str(self.embeddings_dir / "faiss_index.bin"))
            
            logger.info("‚úÖ FAISS index created and saved")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to create FAISS index: {e}")
            return False
    
    def train_tfidf_vectorizer(self):
        """Train TF-IDF vectorizer for keyword search"""
        logger.info("üîÑ Training TF-IDF vectorizer...")
        
        try:
            # Prepare texts
            texts = []
            for doc in self.legal_documents:
                texts.append(doc["content"])
            
            # Train TF-IDF
            self.vectorizer = TfidfVectorizer(
                max_features=10000,
                stop_words='english',
                ngram_range=(1, 2)
            )
            
            tfidf_matrix = self.vectorizer.fit_transform(texts)
            
            # Save vectorizer and matrix
            with open(self.models_dir / "tfidf_vectorizer.pkl", "wb") as f:
                pickle.dump(self.vectorizer, f)
            
            np.save(self.embeddings_dir / "tfidf_matrix.npy", tfidf_matrix.toarray())
            
            logger.info("‚úÖ TF-IDF vectorizer trained and saved")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to train TF-IDF vectorizer: {e}")
            return False
    
    def create_enhanced_app(self):
        """Create enhanced app with all trained components"""
        logger.info("üîÑ Creating enhanced app...")
        
        app_code = '''#!/usr/bin/env python3
"""
A-Qlegal 2.0 - Enhanced Version with All Data
Advanced legal AI assistant with comprehensive training
"""

import json
import streamlit as st
import numpy as np
import faiss
import pickle
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from pathlib import Path
import re

# Load trained models
@st.cache_resource
def load_models():
    """Load all trained models"""
    try:
        # Load sentence transformer
        sentence_model = SentenceTransformer('models/sentence_transformer')
        
        # Load FAISS index
        faiss_index = faiss.read_index('data/embeddings/faiss_index.bin')
        
        # Load TF-IDF vectorizer
        with open('models/tfidf_vectorizer.pkl', 'rb') as f:
            tfidf_vectorizer = pickle.load(f)
        
        # Load TF-IDF matrix
        tfidf_matrix = np.load('data/embeddings/tfidf_matrix.npy')
        
        return sentence_model, faiss_index, tfidf_vectorizer, tfidf_matrix
    except Exception as e:
        st.error(f"Failed to load models: {e}")
        return None, None, None, None

# Load legal data
@st.cache_data
def load_legal_data():
    """Load processed legal data"""
    try:
        with open("data/processed/all_legal_documents.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return []

def semantic_search(query, sentence_model, faiss_index, data, top_k=5):
    """Perform semantic search using sentence transformer"""
    try:
        # Encode query
        query_embedding = sentence_model.encode([query])
        faiss.normalize_L2(query_embedding)
        
        # Search FAISS index
        scores, indices = faiss_index.search(query_embedding, top_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(data):
                doc = data[idx]
                doc['similarity_score'] = float(score)
                results.append(doc)
        
        return results
    except Exception as e:
        st.error(f"Semantic search failed: {e}")
        return []

def keyword_search(query, tfidf_vectorizer, tfidf_matrix, data, top_k=5):
    """Perform keyword search using TF-IDF"""
    try:
        # Transform query
        query_vector = tfidf_vectorizer.transform([query])
        
        # Calculate similarities
        similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
        
        # Get top results
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0:
                doc = data[idx]
                doc['similarity_score'] = float(similarities[idx])
                results.append(doc)
        
        return results
    except Exception as e:
        st.error(f"Keyword search failed: {e}")
        return []

def hybrid_search(query, sentence_model, faiss_index, tfidf_vectorizer, tfidf_matrix, data, top_k=5):
    """Combine semantic and keyword search"""
    # Semantic search
    semantic_results = semantic_search(query, sentence_model, faiss_index, data, top_k)
    
    # Keyword search
    keyword_results = keyword_search(query, tfidf_vectorizer, tfidf_matrix, data, top_k)
    
    # Combine and deduplicate
    all_results = {}
    
    for doc in semantic_results:
        doc_id = doc.get('id', doc.get('title', ''))
        all_results[doc_id] = doc
    
    for doc in keyword_results:
        doc_id = doc.get('id', doc.get('title', ''))
        if doc_id in all_results:
            # Average the scores
            all_results[doc_id]['similarity_score'] = (
                all_results[doc_id]['similarity_score'] + doc['similarity_score']
            ) / 2
        else:
            all_results[doc_id] = doc
    
    # Sort by score and return top results
    results = list(all_results.values())
    results.sort(key=lambda x: x['similarity_score'], reverse=True)
    
    return results[:top_k]

def main():
    st.set_page_config(
        page_title="A-Qlegal 2.0 - Enhanced",
        page_icon="‚öñÔ∏è",
        layout="wide"
    )
    
    st.title("‚öñÔ∏è A-Qlegal 2.0 - Enhanced Legal AI Assistant")
    st.markdown("**Your comprehensive AI-powered legal assistant trained on extensive Indian law data**")
    
    # Load models and data
    with st.spinner("Loading models and data..."):
        sentence_model, faiss_index, tfidf_vectorizer, tfidf_matrix = load_models()
        data = load_legal_data()
    
    if not data:
        st.error("No legal data found. Please run the training script first.")
        return
    
    # Sidebar
    with st.sidebar:
        st.header("üìö Legal Database")
        st.metric("Total Documents", len(data))
        
        # Category breakdown
        categories = {}
        for doc in data:
            cat = doc.get('category', 'Unknown')
            categories[cat] = categories.get(cat, 0) + 1
        
        st.header("üìä Categories")
        for cat, count in sorted(categories.items()):
            st.write(f"‚Ä¢ {cat}: {count}")
        
        st.header("üîç Search Options")
        search_type = st.selectbox(
            "Search Type",
            ["Hybrid (Recommended)", "Semantic", "Keyword"]
        )
        
        top_k = st.slider("Number of Results", 1, 20, 5)
    
    # Main interface
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("üí¨ Ask a Legal Question")
        query = st.text_input(
            "Enter your legal question:",
            placeholder="e.g., What is the punishment for fraud?",
            key="query_input"
        )
        
        if st.button("üîç Search", type="primary"):
            if query:
                with st.spinner("Searching..."):
                    if search_type == "Semantic":
                        results = semantic_search(query, sentence_model, faiss_index, data, top_k)
                    elif search_type == "Keyword":
                        results = keyword_search(query, tfidf_vectorizer, tfidf_matrix, data, top_k)
                    else:  # Hybrid
                        results = hybrid_search(query, sentence_model, faiss_index, tfidf_vectorizer, tfidf_matrix, data, top_k)
                
                if results:
                    st.success(f"Found {len(results)} relevant legal document(s)")
                    
                    for i, doc in enumerate(results, 1):
                        with st.expander(f"{i}. {doc.get('title', 'Unknown')} - {doc.get('category', 'Unknown')} (Score: {doc.get('similarity_score', 0):.3f})", expanded=i==1):
                            if doc.get('section'):
                                st.subheader(f"üìñ {doc['section']}")
                            
                            if doc.get('content'):
                                st.subheader("üìÑ Legal Text")
                                st.write(doc['content'])
                            
                            if doc.get('simplified_summary'):
                                st.subheader("üìù Simplified Summary")
                                st.write(doc['simplified_summary'])
                            
                            if doc.get('real_life_example'):
                                st.subheader("üè† Real-Life Example")
                                st.write(doc['real_life_example'])
                            
                            if doc.get('punishment'):
                                st.subheader("‚öñÔ∏è Punishment")
                                st.write(f"**{doc['punishment']}**")
                            
                            if doc.get('keywords'):
                                st.subheader("üè∑Ô∏è Keywords")
                                st.write(", ".join(doc['keywords']))
                            
                            if doc.get('citations'):
                                st.subheader("üìö Citations")
                                st.write(", ".join(doc['citations']))
                else:
                    st.warning("No relevant legal documents found. Try different keywords.")
            else:
                st.warning("Please enter a question.")
    
    with col2:
        st.header("üìä Statistics")
        st.metric("Total Legal Documents", len(data))
        st.metric("Categories", len(categories))
        
        st.header("üí° Search Tips")
        st.info("""
        **Enhanced Search Features:**
        
        üîç **Hybrid Search**: Combines semantic and keyword search for best results
        
        üìö **Comprehensive Database**: Trained on extensive Indian legal data
        
        üéØ **Smart Ranking**: Results ranked by relevance and similarity
        
        üìñ **Rich Context**: Legal text, simplified summaries, and examples
        
        ‚öñÔ∏è **Complete Information**: Punishments, citations, and keywords
        """)
        
        st.header("üöÄ Advanced Features")
        st.success("""
        ‚úÖ **Semantic Understanding**: AI understands meaning, not just keywords
        
        ‚úÖ **Multi-Source Data**: IPC, CrPC, Constitution, Court judgments
        
        ‚úÖ **Real-Time Search**: Fast retrieval from large legal database
        
        ‚úÖ **Contextual Answers**: Comprehensive legal information
        
        ‚úÖ **User-Friendly**: Simple interface for complex legal queries
        """)

if __name__ == "__main__":
    main()
'''
        
        with open("enhanced_legal_app.py", "w", encoding="utf-8") as f:
            f.write(app_code)
        
        logger.info("‚úÖ Enhanced app created")
        return True
    
    def run_training(self):
        """Run complete training pipeline"""
        logger.info("üöÄ Starting A-Qlegal 2.0 Comprehensive Training")
        logger.info("=" * 60)
        
        steps = [
            ("Loading all data", self.load_all_data),
            ("Preprocessing documents", self.preprocess_documents),
            ("Creating Q&A pairs", self.create_qa_pairs),
            ("Training sentence transformer", self.train_sentence_transformer),
            ("Creating FAISS index", self.create_faiss_index),
            ("Training TF-IDF vectorizer", self.train_tfidf_vectorizer),
            ("Creating enhanced app", self.create_enhanced_app)
        ]
        
        for i, (description, func) in enumerate(steps, 1):
            logger.info(f"Step {i}/{len(steps)}: {description}")
            try:
                result = func()
                if result:
                    logger.info(f"‚úÖ {description} completed")
                else:
                    logger.error(f"‚ùå {description} failed")
                    return False
            except Exception as e:
                logger.error(f"‚ùå {description} failed: {e}")
                return False
            logger.info("")
        
        logger.success("üéâ A-Qlegal 2.0 Training Completed Successfully!")
        logger.info("")
        logger.info("üöÄ To run the enhanced app:")
        logger.info("   streamlit run enhanced_legal_app.py")
        logger.info("")
        logger.info("üìä Training Summary:")
        logger.info(f"   ‚Ä¢ Documents processed: {len(self.legal_documents)}")
        logger.info(f"   ‚Ä¢ Q&A pairs created: {len(self.qa_pairs)}")
        logger.info("   ‚Ä¢ Models trained: Sentence Transformer, FAISS, TF-IDF")
        logger.info("   ‚Ä¢ Search types: Semantic, Keyword, Hybrid")
        
        return True

def main():
    """Main function"""
    trainer = AQlegalTrainer()
    success = trainer.run_training()
    
    if success:
        print("\nüéâ Training completed successfully!")
        print("üöÄ Run: streamlit run enhanced_legal_app.py")
    else:
        print("\n‚ùå Training failed. Check logs for details.")

if __name__ == "__main__":
    main()
